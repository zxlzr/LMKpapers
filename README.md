# Dive into Pretrained Language Model Papers

Contributed by **Hongbin Ye**  and **[Ningyu Zhang](https://zxlzr.github.io/)**.

## Papers
1. **Language Models as Knowledge Bases?**. *Petroni, Fabio
Rockt√§schel, Tim*. EMNLP2019.
2. **BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA**
3. **How Can We Know What Language Models Know ?**
4. **Do Nuclear Submarines Have Nuclear Captains ? A Challenge Dataset for Commonsense Reasoning over Adjectives and Objects**
5. **Evaluating Commonsense in Pre-trained Language Models**
6. **How Much Knowledge Can You Pack Into The Parameters of a Language Model?**
